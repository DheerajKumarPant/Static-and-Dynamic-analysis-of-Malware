'''
@author: Dheeraj Kumar Pant , 19111029
'''

import os
import re
import csv
import pandas as pd
import pickle

work_dir = input('Enter the path: ')
current_directory = os.listdir(work_dir)

#These are the features i am using
features = ['NumberOfSections:', 'Characteristics:', 'SizeOfCode:', 'SizeOfInitializedData:',
            'SizeOfImage:', 'SizeOfHeaders:', 'SizeOfStackCommit:', 'NumberOfRelocations:', 'SizeOfRawData:',
            'NumberOfRvaAndSizes:', 'Entropy:']

#These are some valid names(in pe section) that i got while searching through various text files
valid_sec_ = ['.reloc', '.rsrc', '.bss', '.exc', '.crt', '.text', '.data', '.idata', '.rdata', '.tls', '.code',
'reloc', 'rsrc', 'bss', 'exc', 'crt', 'text', 'data', 'idata', 'rdata', 'tls', 'code', '.ndata', 'ndata', 'epcl_set',
'epcl_tes', 'epcl_tex', 'epcl_ini','epcl_con','wixburn','.epcl_set', '.epcl_tes', '.epcl_tex', '.epcl_ini','.epcl_con','.wixburn']

#This will contain final data-set with all data points
final_features={}

#This will contain hash of files to be analyzed
hash_of_files=[]
hash_of_files.append('File_Hash')

#Name and DLL count(DLLC) were important features for me so i included them
final_features['Name:']=[]
final_features['DLLC']=[]

#This is creating features from directory import parts under 'directory' section, which are 16 in count, DSIZE here refers to same
NO_OF_DIRECTORIES = 16
for i in range(1,NO_OF_DIRECTORIES+1):
    final_features['DSIZE'+str(i)]=[]

def add_special_directories_size(index,listofstruct):
    """
    This function is just extracting size mentioned under IMAGE_DIRECTORY_ENTRY_EXPORT, under directory section.
    @params : index(index of location from where the feature needs to be taken) 
            : listofstruct(list where actual data is stored)
    """
    for i in range(1,17):
        while listofstruct[index]!='Size:':
            index+=1
        final_features['DSIZE'+str(i)].append(convert_to_int(listofstruct,index+1))
        index+=1
    
def convert_to_int(listofstruct,count):
    """
    This function is converting hex to decimal
    @params : listofstruct(list where actual data is stored)
            : count(used for indexing purpose)
    @return : float
    """
    value=0.0
    if listofstruct[count][0:2] == "0x":
        value = int(listofstruct[count], 16)
    else:
        value = float(listofstruct[count])
    return value



#x is my current working directory so now i am iterating through it using 'dir'
for direc in current_directory:
    
    #apppending hash of file
    hash_of_files.append(direc)
    
    
    #opening directory
    struct_info = open(os.path.join(work_dir, direc) + '\\Structure_Info.txt', 'r',encoding="latin1")

    #This section is removing extra space from data and converting data into list form    
    listofstruct = str(struct_info.read())
    remove_xtra_space = re.sub('\s+', ' ', listofstruct)
    listofstruct = remove_xtra_space.split(" ")

    #Some variable which are used in future as flags or condition check
    count = 0
    is_dir_sec = 0
    pe_count = 0
    value = 0.0
    count_dlls = 0
    validname = 1

    #unique_feature_map will count multiple occurances of same strings in list
    unique_features_map = {}

    #In this section i am taking features like 'characteristic' and 'entropy' three times(as they are occcuring multiple times)
    for itr in features:
        if itr=='Characteristics:':
            unique_features_map[itr] = 3
        elif itr=='Entropy:':
            unique_features_map[itr] = 3
        else:
            unique_features_map[itr] = 1


    #Here i am iterating through the list which i generated above
    for i in listofstruct:

        #Here i am extracting sizes by calling add_special_directories_size
        if i=='----------Directories----------':
            add_special_directories_size(count,listofstruct)
            is_dir_sec = 1
            count+=1

        #Here i am extracting features and appending them to final_feature
        if i in features and unique_features_map[i]:

            value=convert_to_int(listofstruct,count+1)
            if i == 'NumberOfSections:':
                pe_count = value
            #For creating unique keys for every features(special for multiple occuring features)
            key = i[:-1] + str(unique_features_map[i])

            if final_features.get(key, -1) == -1:
                final_features[key] = []
            final_features[key].append(value)
            
            if unique_features_map[i]>0:
                unique_features_map[i]-=1
        else:
            if i=='Name:' and validname and pe_count:
                if listofstruct[count + 1].lower() not in valid_sec_:
                    validname = 0
                pe_count -= 1
            if i=='[IMAGE_IMPORT_DESCRIPTOR]':
                # print(count_dlls)
                count_dlls+=1

        count += 1

    final_features['Name:'].append(validname)
    
    #Extracting DLL count features which require seperate iteration , and filling zeroes in places where i got no data!
    final_features['DLLC'].append(count_dlls)
    for itr in features:
        if unique_features_map[itr]!=0:
            for itr2 in range(unique_features_map[itr],0,-1):
                key = itr[:-1] + str(itr2)
                if final_features.get(key, -1) == -1:
                    final_features[key] = []
                final_features[key].append(0)

    if is_dir_sec == 0:
        for i in range(1,17):
            final_features['DSIZE'+str(i)].append(0)

#Converting final features to DataFrame
final_features=pd.DataFrame.from_dict(final_features)

#Loading my Decision Tree model
static_model = pickle.load(open('STATIC_MODEL','rb'))

#labels that are predict by above model is inside predicted_labels
predicted_labels=[]
predicted_labels.append('Predicted Label')
predicted_labels[1:] = list(static_model.predict(final_features))

#This portion of code is just converting whole data into csv
with open('Static.csv', 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerows(zip(hash_of_files, predicted_labels))